1.过程
  数据准备
  数据切片
  数据集的预处理
  数据的batching方法
  Seq2Seq模型的代码实现
    模型训练
    解码或推理程序
  Attention机制（注意力机制）
  总结
2.数据准备
  一个TED 演讲的中英字幕。
  下载地址：https://wit3.fbk.eu/mt.php?release=2015-01
3.数据切片
  我们得到的文件里面都是自然语言，“今天天气很好。”这样的句子。我们首先要做的就是要将这些句子里的每一个字以及标点符号，用空格隔开。所以第一步就是利用工具进行文本切片。
  用正则表达式的方法，去除了这些介绍部分的文字。英文和中文只需要改变名字和路径就行了
4.数据集的预处理
  为了将文本转化为模型可以读入的单词序列，需要将这4000个中文词汇，10000个英文词汇分别映射到0~9999之间的整数编号。
  首先按照词频顺序确定词汇表，然后将词汇表保存到两个独立的vocab的文件中。
  在确定了词汇表之后，再讲训练文件、测试文件等都根据词汇文件转化为单词编号。每个单词的编号就是它在词汇文件的行号。
  这里只加<eos>是有道理的！，在下面batching的时候，会说为什么数据预处理的时候只在每个句子的后面加<eos>！
5. 数据的batching方法
  在机器翻译的训练样本中，每个句子对通常都是作为独立的数据来训练的。由于每个句子的长短不一致，因此在将这些句子放到同一个batch时，需要将较短的句子补齐到与同 batch 内最长句子相同的长度。用于填充长度而填入的位置叫做填充（padding）。
  在TensorFlow中，tf.data.Dataset 的 padded_batch 函数可以解决这个问题。
  循环神经网络在读取数据时会将填充位置的内容与其他内容一样纳入计算，因此为了不让填充影响训练，可能会影响训练结果和loss的计算，
  所以需要以下两个解决对策：
   :第一，循环神经网络在读取填充时，应当跳过这一位置的计算。以编码器为例，如果编码器在读取填充时，像正常输入一样处理填充输入，
     那么在读取"B1B200”之后产生的最后一位隐藏序列就和读取“B1B2”之后的隐藏状态不同，会产生错误的结果。通俗一点来说就是通过编码器预测，输入原始数据＋padding数据产生的结果变了。
     但是TensorFlow提供了 tf.nn.dynamic_rnn函数来很方便的实现这一功能，解决这个问题。dynamic_rnn 对每一个batch的数据读取两个输入。
       输入数据的内容（维度为[batch_size, time]）
       输入数据的内容（维度为[batch_size, time]）
     对于输入batch里的每一条数据，在读取了相应长度的内容后，dynamic_rnn就跳过后面的输入，直接把前一步的计算结果复制到后面的时刻。这样可以保证padding是否存在不影响模型效果。通俗来说就是用一个句子的长度也就是time来把控这一点。
     并且使用dynamic_rnn时每个batch的最大序列长度不需要相同。在训练中dynamic_rnn会根据每个batch的最大长度动态展开到需要的层数，其实就是对每个batch本身的最大长度没有关系，函数会自动动态(dynamic)调整。
   :第二，在设计损失函数时需要特别将填充位置的损失权重设置为 0 ,这样在填充位置产生的预测不会影响梯度的计算。
  代码使用tf.data.Dataset.padded_batch 来进行填充和 batching，并记录每个句子的序列长度以用作dynamic_rnn的输入。
  这里没有将所有的数据读入内存，而是使用Dataset从磁盘动态读取数据。
 6.Seq2Seq模型
  6.1 模型训练
    LSTM 作为循环神经网络的主体,并在 Softmax 层和词向量层之间共享参数，增加如下：
    增加了一个循环神经网络作为编码器
    使用 Dataset 动态读取数据，而不是直接将所有数据读入内存（这个就是Dataset输入数据的特点）
    每个 batch 完全独立，不需要在batch之间传递状态（因为不是一个文件整条句子，每个句子之间没有传递关系）
    每训练200步便将模型参数保存到一个 checkpoint 中，以后用于测试。
   词向量:
     # 为源语言和目标语言分别定义词向量(这部分代码定义的就是第一种词向量要乘的矩阵，shape是[词汇表大小，隐藏层]。)
       self.src_embedding = tf.get_variable("src_emb", [SRC_VOCAB_SIZE, HIDDEN_SIZE]) 
       self.trg_embedding = tf.get_variable("trg_emb", [TRG_VOCAB_SIZE, HIDDEN_SIZE])
     首先，词向量有两种，一种是One-Hot Encoder，也就是以前常用的，当然现在也有用。
     第二种，就是稠密向量，因为第一种每个词向量也太大了，如果词汇表有10000的话，那么对于某个词来说，它的词向量就是，一个1x10000的矩阵，其中的一个是1，其他9999都是0。
     那么这第二种要怎么得到呢？（我的理解啊）就是用第一种方式转化得到的。第一种方法得到的词向量作为输入，然后经过一个10000 x hidden_layer（隐藏层大小）的矩阵，矩阵乘法得到1 x hidden_layer 的稠密矩阵，这个矩阵就是我所说的第二种词向量。
     # 将输入和输出单词编号转为词向量(这一部分代码是计算第二种词向量的过程，虽然用的函数时tf.nn.embedding_lookup，实际上也就是矩阵乘法的意思。)
       src_emb = tf.nn.embedding_lookup(self.src_embedding, src_input)
       trg_emb = tf.nn.embedding_lookup(self.trg_embedding, trg_input)
  6.2 解码或推理程序
    完成了机器翻译模型的训练，并将训练好的模型保存在checkpoint中。
    怎样从checkpoint中读取模型并对一个新的句子进行翻译。对新输入的句子进行翻译的过程也称为解码或推理。
    在训练的时候解码器是可以从输入读取到完整的目标训练句子。而在解码或推理的过程中模型只能看到输入句子，却看不到目标句子。
    具体过程：
            解码器在第一步读取<sos> 符，预测目标句子的第一个单词，然后需要将这个预测的单词复制到第二步作为输入，再预测第二个单词，直到预测的单词为<eos>为止 。 这个过程需要使用一个循环结构来实现 。
            在TensorFlow 中，循环结构是由 tf.while_loop 来实现的 。
   6.3 如何修改在TensorFlow框架下训练保存的模型参数名称
 7. Attention机制（注意力机制）
  最基本的seq2seq模型包含一个encoder和一个decoder，通常的做法是将一个输入的句子编码成一个固定大小的state，然后作为decoder的初始状态（当然也可以作为每一时刻的输入），但这样的一个状态对于decoder中的所有时刻都是一样的。 
  普通的模型可以看成所有部分的attention都是一样的，而这里的attention-based model对于不同的部分，重要的程度则不同。
  加入attention机制后模型训练：
    ：需要修改的部分主要是把原来编码器的多层rnn结构变成了一个双向的rnn
    : forward函数中的"encoder"和"decoder"部分都做了些许的修改
  要注意，解码的时候初始状态并不是之前编码器最后的隐藏状态，而是什么都没有的初始状态：

    
    
    
    
    
    
    
   
