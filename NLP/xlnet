1.xlnet作为bert的升级模型，主要在以下三个方面进行了优化
    采用AR模型替代AE模型，解决mask带来的负面影响
    双流注意力机制
    引入transformer-xl
  1.1AR与AE语言模型
     目前主流的nlp预训练模型包括两类 autoregressive (AR) language model 与autoencoding (AE) language model，AR模型的主要任务在于评估语料的概率分布，例如，给定一个序列
   X=(x1,...,xT)，AR模型就是在计算其极大似然估计即已知xt之前的序列，预测xt的值，当然也可以反着来即已知xt之后的序列，预测xt的值，看完之后相信你也能发现AR模型的缺点，没错该模型是单向的，
   我们更希望的是根据上下文来预测目标，而不单是上文或者下文，之前open AI提出的GPT就是采用的AR模式，包括GPT2.0也是该模式，那么为什么open ai头要这么铁坚持采用单向模型呢，看完下文你就知道了。
     AE模型采用的就是以上下文的方式，最典型的成功案例就是bert。我们简单回顾下bert的预训练阶段，预训练包括了两个任务，Masked Language Model与Next Sentence Prediction，
    Next Sentence Prediction即判断两个序列的推断关系，Masked Language Model采用了一个标志位[MASK]来随机替换一些词，再用[MASK]的上下文来预测[MASK]的真实值，
    bert的最大问题也是处在这个MASK的点，因为在微调阶段，没有MASK这就导致预训练和微调数据的不统一，从而引入了一些人为误差，我觉得这应该就是为什么GPT坚持采用AR模型的原因。
   在xlnet中，最终还是采用了AR模型，但是怎么解决这个上下文的问题呢，这就是本文的一个重点。
  1.2 排列语言模型
   为了解决上文提到的问题，作者提出了排列语言模型，该模型不再对传统的AR模型的序列的值按顺序进行建模，而是最大化所有可能的序列的因式分解顺序的期望对数似然
   这样处理过后不但保留了序列的上下文信息，也避免了采用mask标记位，巧妙的改进了bert与传统AR模型的缺点。
  1.3 基于目标感知表征的双流自注意力
   虽然排列语言模型能满足目前的目标，但是对于普通的transformer结构来说是存在一定的问题的，
   因为无论预测目标的位置在哪里，因式分解后得到的所有情况都是一样的，并且transformer的权重对于不同的情况是一样的，因此无论目标位置怎么变都能得到相同的分布结果
   这就导致模型没法得到正确的表述，为了解决这个问题，论文中提出来新的分布计算方法，来实现目标位置感知
   论文把该方法称为Two-Stream Self-Attention，双流自注意力，该机制需要解决了两个问题
     如果目标是预测xt，那么只能有其位置信息zt而不能包含内容信息xt
     如果目标是预测其他tokens，那么应该包含xt那么应该包含xzt
   很显然传统的transformer并不满足这样的需求，因此作者采用了两种表述来代替原来的表述，这也是为什么称为双流的原因
     content representation内容表述：，该表述和传统的transformer一样，同时编码了上下文和xt自身
     query representation查询表述，该表述包含上下文的内容信息和目标的位置信息zt，但是不包括目标的内容信息xt，
       K与V的计算并没有包括Q，自然也就无法获取到目标的内容信息，但是目标的位置信息在计算Q的时候保留了下来，
   最后在微调阶段，只需要简单的把query stream移除，只采用content stream即可。
   1.4 集成Transformer-XL
    片段循环机制
    transformer-xl的提出主要是为了解决超长序列的依赖问题，对于普通的transformer由于有一个最长序列的超参数控制其长度，对于特别长的序列就会导致丢失一些信息，transformer-xl就能解决这个问题。
    如果采用transformer-xl，首先取第一个段进行计算，然后把得到的结果的隐藏层的值进行缓存，第二个段计算的过程中，把缓存的值拼接起来再进行计算。该机制不但能保留长依赖关系还能加快训练，因为每一个前置片段都保留了下来，不需要再重新计算，在transformer-xl的论文中，经过试验其速度比transformer快了1800倍。
    相对位置编码
    bert的position embedding采用的是绝对位置编码，但是绝对位置编码在transformer-xl中有一个致命的问题，因为没法区分到底是哪一个片段里的，这就导致了一些位置信息的损失，这里被替换为了transformer-xl中的相对位置编码。
   1.5预训练
    预训练阶段和bert差不多，不过去除了Next Sentence Prediction，作者发现该任务对结果的提升并没有太大的影响。输入的值还是 [A, SEP, B, SEP, CLS]的模式，A与B代表的是两个不同的片段。
   
 
