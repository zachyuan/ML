1.xlnet作为bert的升级模型，主要在以下三个方面进行了优化
    采用AR模型替代AE模型，解决mask带来的负面影响
    双流注意力机制
    引入transformer-xl
  1.1AR与AE语言模型
     目前主流的nlp预训练模型包括两类 autoregressive (AR) language model 与autoencoding (AE) language model，AR模型的主要任务在于评估语料的概率分布，例如，给定一个序列
   X=(x1,...,xT)，AR模型就是在计算其极大似然估计即已知xt之前的序列，预测xt的值，当然也可以反着来即已知xt之后的序列，预测xt的值，看完之后相信你也能发现AR模型的缺点，没错该模型是单向的，
   我们更希望的是根据上下文来预测目标，而不单是上文或者下文，之前open AI提出的GPT就是采用的AR模式，包括GPT2.0也是该模式，那么为什么open ai头要这么铁坚持采用单向模型呢，看完下文你就知道了。
     AE模型采用的就是以上下文的方式，最典型的成功案例就是bert。我们简单回顾下bert的预训练阶段，预训练包括了两个任务，Masked Language Model与Next Sentence Prediction，
    Next Sentence Prediction即判断两个序列的推断关系，Masked Language Model采用了一个标志位[MASK]来随机替换一些词，再用[MASK]的上下文来预测[MASK]的真实值，
    bert的最大问题也是处在这个MASK的点，因为在微调阶段，没有MASK这就导致预训练和微调数据的不统一，从而引入了一些人为误差，我觉得这应该就是为什么GPT坚持采用AR模型的原因。
   在xlnet中，最终还是采用了AR模型，但是怎么解决这个上下文的问题呢，这就是本文的一个重点。
  1.2 排列语言模型
   为了解决上文提到的问题，作者提出了排列语言模型，该模型不再对传统的AR模型的序列的值按顺序进行建模，而是最大化所有可能的序列的因式分解顺序的期望对数似然
   这样处理过后不但保留了序列的上下文信息，也避免了采用mask标记位，巧妙的改进了bert与传统AR模型的缺点。
  1.3 基于目标感知表征的双流自注意力
   虽然排列语言模型能满足目前的目标，但是对于普通的transformer结构来说是存在一定的问题的，
   因为无论预测目标的位置在哪里，因式分解后得到的所有情况都是一样的，并且transformer的权重对于不同的情况是一样的，因此无论目标位置怎么变都能得到相同的分布结果
   这就导致模型没法得到正确的表述，为了解决这个问题，论文中提出来新的分布计算方法，来实现目标位置感知
   论文把该方法称为Two-Stream Self-Attention，双流自注意力，该机制需要解决了两个问题
     如果目标是预测xt，那么只能有其位置信息zt而不能包含内容信息xt
     如果目标是预测其他tokens，那么应该包含xt那么应该包含zt
   很显然传统的transformer并不满足这样的需求，因此作者采用了两种表述来代替原来的表述，这也是为什么称为双流的原因
     content representation内容表述：，该表述和传统的transformer一样，同时编码了上下文和xt自身
     query representation查询表述，该表述包含上下文的内容信息和目标的位置信息zt，但是不包括目标的内容信息xt，
       K与V的计算并没有包括Q，自然也就无法获取到目标的内容信息，但是目标的位置信息在计算Q的时候保留了下来，
   最后在微调阶段，只需要简单的把query stream移除，只采用content stream即可。
   1.4 集成Transformer-XL
    片段循环机制
    transformer-xl的提出主要是为了解决超长序列的依赖问题，对于普通的transformer由于有一个最长序列的超参数控制其长度，对于特别长的序列就会导致丢失一些信息，transformer-xl就能解决这个问题。
    如果采用transformer-xl，首先取第一个段进行计算，然后把得到的结果的隐藏层的值进行缓存，第二个段计算的过程中，把缓存的值拼接起来再进行计算。该机制不但能保留长依赖关系还能加快训练，因为每一个前置片段都保留了下来，不需要再重新计算，在transformer-xl的论文中，经过试验其速度比transformer快了1800倍。
    相对位置编码
    bert的position embedding采用的是绝对位置编码，但是绝对位置编码在transformer-xl中有一个致命的问题，因为没法区分到底是哪一个片段里的，这就导致了一些位置信息的损失，这里被替换为了transformer-xl中的相对位置编码。
   1.5预训练
    预训练阶段和bert差不多，不过去除了Next Sentence Prediction，作者发现该任务对结果的提升并没有太大的影响。输入的值还是 [A, SEP, B, SEP, CLS]的模式，A与B代表的是两个不同的片段。
 ------------
1.摘要
   BERT 这样基于去噪自编码器的预训练模型可以很好地建模双向语境信息，性能优于基于自回归语言模型的预训练方法。然而，由于需要 mask 一部分输入，BERT 忽略了被 mask 位置之间的依赖关系，因此出现预训练和微调效果的差异（pretrain-finetune discrepancy）
   基于这些优缺点，该研究提出了一种泛化的自回归预训练模型 XLNet。XLNet 可以：1）通过最大化所有可能的因式分解顺序的对数似然，学习双向语境信息；2）用自回归本身的特点克服 BERT 的缺点。此外，XLNet 还融合了当前最优自回归模型 Transformer-XL 的思路。
   最终，XLNet 在 20 个任务上超过了 BERT 的表现，并在 18 个任务上取得了当前最佳效果（state-of-the-art），包括机器问答、自然语言推断、情感分析和文档排序。
   1.1 AR 与 AE 两大阵营
    AR语言模型是一种使用上下文词来预测下一个词的模型。但是在这里，上下文单词被限制在两个方向，前向或后向。GPT 和 GPT-2 都 AR 语言模型。
    AR 语言模型的优势是擅长生成式自然语言处理任务。 因为在生成上下文时，通常是前向的。AR 语言模型很自然地适用于此类 NLP 任务。
    但AR语言模型有一些缺点，它只能使用前向上下文或后向上下文，这意味着它不能同时使用前向和后向上下文。
   1.2 XLNet和BERT有什么区别
    与 AR 语言模型不同，BERT 被归类为自动编码器（AE）语言模型。AE 语言模型旨在从损坏的输入重建原始数据。
    损坏的输入意味着我们在预训练阶段用 [MASK] 替换原始词 into。目标是预测 into 得到原始句子。
    AE 语言模型的优势是，它可以从向前和向后的方向看到上下文。
    但 AE 语言模型也有其缺点。它在预训练中使用 [MASK]，但这种人为的符号在调优时在真实数据中并不存在，会导致预训练-调优的差异。[MASK] 的另一个缺点是它假设预测（掩蔽的）词 在给定未屏蔽的 词 的情况下彼此独立。
    但是我们知道模型应该学习预测（掩蔽）词之间的这种相关性来预测其中一个词。
    XLNet 提出了一种让 AR 语言模型从双向上下文中学习的新方法，以避免 MASK 方法在 AE 语言模型中带来的缺点。
   1.3 两大阵营间需要新的 XLNet
    现有的语言预训练目标各有优劣，这篇新研究提出了一种泛化自回归方法 XLNet，既集合了 AR 和 AE 方法的优势，又避免了二者的缺陷。
    首先，XLNet 不使用传统 AR 模型中固定的前向或后向因式分解顺序，而是最大化所有可能因式分解顺序的期望对数似然。
    由于对因式分解顺序的排列操作，每个位置的语境都包含来自左侧和右侧的 token。因此，每个位置都能学习来自所有位置的语境信息，即捕捉双向语境。
    其次，作为一个泛化 AR 语言模型，XLNet 不依赖残缺数据。因此，XLNet 不会有 BERT 的预训练-微调差异。同时，自回归目标提供一种自然的方式，来利用乘法法则对预测 token 的联合概率执行因式分解（factorize），这消除了 BERT 中的独立性假设。
    除了提出一个新的预训练目标，XLNet 还改进了预训练的架构设计。
    受到 AR 语言建模领域最新进展的启发，XLNet 将 Transformer-XL 的分割循环机制（segment recurrence mechanism）和相对编码范式（relative encoding）整合到预训练中，实验表明，这种做法提高了性能，尤其是在那些包含较长文本序列的任务中。
    简单地使用 Transformer(-XL) 架构进行基于排列的（permutation-based）语言建模是不成功的，因为因式分解顺序是任意的、训练目标是模糊的。因此，研究人员提出，对 Transformer(-XL) 网络的参数化方式进行修改，移除模糊性。
2.被提议的方法
 2.1 背景 
  这两个模型的优缺点分别为：独立假设,输入噪声,双向上下文
 2.2目标：排列语言建模
  具体来说，一个长度为 T 的序列 x 拥有 T! 种不同的排序方式，可以执行有效的自回归因式分解。从直觉上来看，如果模型参数在所有因式分解顺序中共享，那么预计模型将学习从两边的所有位置上收集信息。
  排列语言模型的目标是调整模型参数使得似然概率最大
  注意：上面的模型只会遍历概率的分解顺序，并不会改变原始词的顺序。
 2.3 模型架构：对目标感知表征的双流自注意力
 例子一：
   前面我们为预训练语言模型构建了新的任务目标，这里就需要调整 Transformer 以适应任务。如果读者了解一些 Transformer，那么就会知道某个 Token 的内容和位置向量在输入到模型前就已经加在一起了，后续的隐向量同时具有内容和位置的信息。
   「新任务希望在预测下一个词时只能提供位置信息，不能提供内容相关的信息。因此模型希望同时做两件事，首先它希望预测自己到底是哪个字符，其次还要能预测后面的字符是哪个。」
   这类似于条件句：如果模型预测当前词，则只能使用位置向量；如果模型预测后续的词，那么使用位置加内容向量。
   Transformer 中每个词的表征由其词向量和位置编码共同决定 -- 我们既拿到了词本身的性质，又有词的位置信息。所以 Transformer 天然就和乱序语言模型相契合。
  例子二：
   假设整句话为 ["我 1", "今天 2", "很 3",「开心 4」]，我们只采样出一个样本 (["今天 2", "很 3", "开心 4"] → "我 1" )，XLNet 的做法和 BERT 有同有异。
   和 BERT 一样，XLNet 同样是将目标词 "我 1" 替换成一个特殊字符 "MASK1"。和 BERT 不同，"MASK" 不会纳入表征的地址向量 k 以及内容向量 v 的计算，"MASK" 自始至终只充当了查询向量 q 的角色，因此所有词的表征中都不会拿到 "MASK" 的信息。这也杜绝了 "MASK" 的引入带来的预训练-微调差异 (Pretrain-Finetune Discrepancy) -- 这个改动也可以直接应用到 BERT 上面。
   我们还希望保证训练效率 -- 我们想和自回归语言模型一样，只进行一次整句的表征计算便可以获得所有样本的语境表征。这时所有词的表征就必须同时计算，此时便有标签泄露带来的矛盾：对于某个需要预测的目标词，我们既需要得到包含它信息以及位置的表征 h (用来进一步计算其他词的表征)，又需要得到不包含它信息，只包含它位置的表征 g (用来做语境的表征)。
   一个很自然的想法就是同时计算两套表征，这便是 XLNet 提出的双通道自注意力 (Two Stream Self-Attention)，同时计算内容表征通道 (Content Stream) h 和语境表征通道 (Query Stream) g。
  
3.讨论与分析
  3.1与BERT比较
   XLNet和BERT都是预测一个句子的部分词，但是背后的原因是不同的。BERT使用的是Mask语言模型，因此只能预测部分词(总不能把所有词都Mask了然后预测?)。而XLNet预测部分词是出于性能考虑，而BERT是随机的选择一些词来预测。
   除此之外，它们最大的区别其实就是BERT是约等号，也就是条件独立的假设——那些被MASK的词在给定非MASK的词的条件下是独立的。但是我们前面分析过，这个假设并不(总是)成立。
  3.2XLNet 的模型改进增益
   

