1.BERT主要特点
 使用了Transformer作为算法的主要框架，Trabsformer能更彻底的捕捉语句中的双向关系；
 使用了Mask Language Model(MLM) 和 Next Sentence Prediction(NSP) 的多任务训练目标；
 使用更强大的机器训练更大规模的数据，使BERT的结果达到了全新的高度，并且Google开源了BERT模型，用户可以直接使用BERT作为Word2Vec的转换矩阵并高效的将其应用到自己的任务中。
 本质
 BERT的本质上是通过在海量的语料的基础上运行自监督学习方法为单词学习一个好的特征表示，所谓自监督学习是指在没有人工标注的数据上运行的监督学习。
 在以后特定的NLP任务中，我们可以直接使用BERT的特征表示作为该任务的词嵌入特征。所以BERT提供的是一个供其它任务迁移学习的模型，该模型可以根据任务微调或者固定之后作为特征提取器。
2.网络架构
 BERT的网络架构使用的是《Attention is all you need》中提出的多层Transformer结构，其最大的特点是抛弃了传统的RNN和CNN，通过Attention机制将任意位置的两个单词的距离转换成1，有效的解决了NLP中棘手的长期依赖问题。
 Transformer是一个encoder-decoder的结构，由若干个编码器和解码器堆叠形成。
 编码器，由Multi-Head Attention和一个全连接组成，用于将输入语料转化成特征向量。解码器，其输入为编码器的输出以及已经预测的结果，由Masked Multi-Head Attention, Multi-Head Attention以及一个全连接组成，用于输出最后结果的条件概率。
 BERT提供了简单和复杂两个模型。
 优点是只有BERT表征会基于所有层中的左右两侧语境。BERT能做到这一点得益于Transformer中Attention机制将任意位置的两个单词的距离转换成了1。
 2.1 输入表示
  BERT的输入的编码向量（长度是512）是3个嵌入特征的单位和
  WordPiece 嵌入：WordPiece是指将单词划分成一组有限的公共子词单元，能在单词的有效性和字符的灵活性之间取得一个折中的平衡。例如示例中‘playing’被拆分成了‘play’和‘ing’；
  位置嵌入（Position Embedding）：位置嵌入是指将单词的位置信息编码成特征向量，位置嵌入是向模型中引入单词位置关系的至关重要的一环。
  分割嵌入（Segment Embedding）：用于区分两个句子，例如B是否是A的下文（对话场景，问答场景等）。对于句子对，第一个句子的特征值是0，第二个句子的特征值是1。
 最后，说明一下两个特殊符号[CLS]和[SEP]，其中[CLS]表示该特征用于分类模型，对非分类模型，该符合可以省去。[SEP]表示分句符号，用于断开输入语料中的两个句子。
 2.2 预训练任务
  BERT是一个多任务模型，它的任务是由两个自监督任务组成，即MLM和NSP。
  2.2.1 Task #1： Masked Language Model
  所谓MLM是指在训练的时候随即从输入预料上mask掉一些单词，然后通过的上下文预测该单词，该任务非常像我们在中学时期经常做的完形填空。正如传统的语言模型算法和RNN匹配那样，MLM的这个性质和Transformer的结构是非常匹配的。
  在BERT的实验中，15%的WordPiece Token会被随机Mask掉。在训练模型时，一个句子会被多次喂到模型中用于参数学习，
  但是Google并没有在每次都mask掉这些单词，而是在确定要Mask掉的单词之后，80%的时候会直接替换为[Mask]，10%的时候将其替换为其它任意单词，10%的时候会保留原始Token。
  这么做的原因是如果句子中的某个Token100%都会被mask掉，那么在fine-tuning的时候模型就会有一些没有见过的单词。
  加入随机Token的原因是因为Transformer要保持对每个输入token的分布式表征，否则模型就会记住这个[mask]是token ’hairy‘。
  至于单词带来的负面影响，因为一个单词被随机替换掉的概率只有15%*10% =1.5%，这个负面影响其实是可以忽略不计的。
  [另外文章指出每次只预测15%的单词，因此模型收敛的比较慢。]
  2.2.2 Task #2: Next Sentence Prediction
  加入随机Token的原因是因为Transformer要保持对每个输入token的分布式表征，否则模型就会记住这个[mask]是token ’hairy‘。
  至于单词带来的负面影响，因为一个单词被随机替换掉的概率只有15%*10% =1.5%，这个负面影响其实是可以忽略不计的。
  Next Sentence Prediction（NSP）的任务是判断句子B是否是句子A的下文。如果是的话输出’IsNext‘，否则输出’NotNext‘。
  训练数据的生成方式是从平行语料中随机抽取的连续两句话，其中50%保留抽取的两句话，它们符合IsNext关系，另外50%的第二句话是随机从预料中提取的，它们的关系是NotNext的。这个关系保存在图中的[CLS]符号中。
 2.3 微调
  在海量单预料上训练完BERT之后，便可以将其应用到NLP的各个任务中了。
  对于其它任务来说，我们也可以根据BERT的输出信息作出对应的预测。
  只需要在BERT的基础上再添加一个输出层便可以完成对特定任务的微调。
  句子对的分类任务;(基于句子对的分类任务,基于单个句子的分类任务)
  对于GLUE数据集的分类任务（MNLI，QQP，QNLI，SST-B，MRPC，RTE，SST-2，CoLA），BERT的微调方法是根据[CLS]标志生成一组特征向量 C ，并通过一层全连接进行微调。
  损失函数根据任务类型自行设计，例如多分类的softmax或者二分类的sigmoid。
  SWAG的微调方法与GLUE数据集类似，只不过其输出是四个可能选项的softmax。
  问答任务:
  SQuAD v1.1：给定一个句子（通常是一个问题）和一段描述文本，输出这个问题的答案，类似于做阅读理解的简答题。如图5.(c)表示的，SQuAD的输入是问题和描述文本的句子对。
  输出是特征向量，通过在描述文本上接一层激活函数为softmax的全连接来获得输出文本的条件概率，全连接的输出节点个数是语料中Token的个数。
  命名实体识别:
  CoNLL-2003 NER：判断一个句子中的单词是不是Person，Organization，Location，Miscellaneous或者other（无命名实体）。微调CoNLL-2003 NER时将整个句子作为输入，在每个时间片输出一个概率，并通过softmax得到这个Token的实体类别。
-----------
1.BERT模型
  BERT的全称是Bidirectional Encoder Representation from Transformers，即双向Transformer的Encoder，
  因为decoder是不能获要预测的信息的。模型的主要创新点都在pre-train方法上，即用了Masked LM和Next Sentence Prediction两种方法分别捕捉词语和句子级别的representation。
2.模型结构GPT EMLO Bert
  对比OpenAI GPT(Generative pre-trained transformer)，BERT是双向的Transformer block连接；就像单向rnn和双向rnn的区别，直觉上来讲效果会好一些。
  对比ELMo，虽然都是“双向”，但目标函数其实是不同的。ELMo是分别作为目标函数，独立训练处两个representation然后拼接，而BERT则是以整个作为目标函数训练LM。
  2.1 Embedding
   Token Embeddings是词向量，第一个单词是CLS标志，可以用于之后的分类任务
   Segment Embeddings用来区别两种句子，因为预训练不光做LM还要做以两个句子为输入的分类任务
   Position Embeddings和之前文章中的Transformer不一样，不是三角函数而是学习出来的
  2.2 Pre-training Task 1#: Masked LM
   在训练过程中作者随机mask 15%的token，而不是把像cbow一样把每个词都预测一遍。最终的损失函数只计算被mask掉那个token。
   Masked LM预训练阶段模型是不知道真正被mask的是哪个词，所以模型每个词都要关注。
  2.3 Pre-training Task 2#: Next Sentence Prediction
   目的是让模型理解两个句子之间的联系。
   特意说了语料的选取很关键，要选用document-level的而不是sentence-level的，这样可以具备抽象连续长序列特征的能力。
---------------
1.BERT的用途
  BERT模型是一个预训练语言表示模型，所谓预训练语言表示模型，就是先用这个模型在可与最终任务无关的大数据集上训练处语言的表示，然后将学到的知识（表示）用到任务相关的语言表示上。
  这样做的原因是考虑到（1）若任务相关的数据集可能很小，小数据无法反映出语言间的复杂关系，同样也很容易让复杂的深度网络模型过拟合；
                    （2）若任务相关的数据集很大，大数据上的训练时间很长，要在短时间内、特别是有限计算资源下利用深度网络学到相关的信息是困难的。
  提出BERT模型的论文将预训练语言表示模型根据实际使用时的策略也分为两类，一类是基于特征的方法（feature-based approach），另一类是基于微调的方法（fine-tuning approach）。
  基于特征的方法利用预训练好的模型提取文本特征，并将所提取的特征作为额外的特征加入到针对特定任务的表示模型之中。之前提到的ELMo就是基于特征的方法中的典型代表。
  基于微调的方法则是使用特定任务的数据集和标签来微调预训练好的模型（网络）参数，从而使得预训练的模型能够适应特定任务。
  首先解读的BERT模型就属于基于微调方法的模型。
2.BERT的优势
  当前语言模型的主要局限是在于它们都是单向的，这限制了预训练中可选的网络结构。
  很早之前就有了双向递归神经网络（Bidirectional RNN），不过呢，即便双向递归神经网络考虑了两个方向的顺序，但是仅仅是独立考虑的。换而言之，ELMo中每一个独立的LSTM都是只考虑了一个方向的上下文，
  即一个词和左右两侧的上下文的关系在这种模型中总总只有一侧被模型捕获到，模型不能捕获这个词同时和左右两侧上下文的关系。由此可见，当前模型所考虑到的语言中的关系是不完备的。
  BERT使用双向转换器，OpenAI GPT使用从左至右的转换器，ELMo使用级联方法连接独立训练的从左至右和从右至左的LSTM。
3.BERT的原理
  BERT模型为了对文本进行双向的建模，采用的不是LSTM等RNN网络有序输入的模式，而是类似于全连接网络（fully-connected network）输入完整句子的模式。从这儿可以看出，BERT所说的双向关系（bi-directional）建模方式，精准的说应该是无向关系（non-directional）建模的关系。
  这儿，可能大家会有一个疑问：CNN和RNN等网络的核心思想就是捕获序关系，因此在图像、文本等有序数据的学习上展现出显著的优势，但这儿为何反其道而行，回到了最初的全连接呢？
  这不得不说到Google团队之前的一项成果转换器（Transformer）。该成果利用基于特别设计的注意力机制（Attention mecanism）的简单全连接网络取代了复杂的CNN和RNN网络，不但大大减少了训练时间，同时有效的提升了网络性能。
  当然，虽然不去显式考虑序关系了，但是每一个词的位置和词与词之间的相对位置关系却是自然语言中最重要的关系之一，决定着一个语句的含义，在建模时绝对不能忽略。
  为了补充这种位置信息，转换器不仅将每个输入的词进行编码，同样对每个词的位置进行编码，然后利用词编码和位置编码之间的运算，将位置信息补充到词编码之中。
  同时，利用注意力机制，动态的学习不同位置的词之间的相互关系。
  说到这儿，我们可以看到，BERT模型其实也并不是第一个对本文进行双向建模的模型。但是，之前的转换器之类的模型却只是用到语句的翻译之中，属于一种有监督的训练模型。
  而BERT模型继承了它双向建模的思想，并继续发扬光大，将其用到了无监督的预训练语言表示学习之中。从有监督到无监督，看似一小步，实则一大步。
  BERT模型吸取了自编码机（auto-encoder）、词编码模型（word2vec）等无监督模型的设计思想，又结合所要捕获的无序关系和句与句关系等信息的特点，提出了对于转换器全新的无监督目标函数。
  BERT模型提出了一种新的预训练目标：掩码语言模型（masked language model）。
  掩码语言模型随机给输入的文字符号打上掩码，即遮盖掉该文字符号，然后用这个文字符号的上下文信息来预测这个文字符号原本的值。
  相对于按照从左到右顺序对文本建模的语言模型，掩码语言模型的训练目标使得所学习到的语言表示可以融合一个文字左右两侧的上下文信息。
  为了逼近真实的数据分布，BERT模型采用了3种不同的打码方式：80%的情况下替换为标识符，10%的情况下替换为随机文字，10%的情况下保持文字符号不变。
  除了使用上述掩码语言模型外，BERT还同时使用了一个后句预测（next sentence prediction）的任务为训练目标，与掩码语言模型一起来进行对语言表示的训练。
  这个设计用以捕获句子与句子之间的相互关系，有利于预训练的通用表示在文本匹配等任务的应用。这也是为何有Segment embeddings了，其实是为了学出前句和后句的位置关系。
  综上，BERT模型的方法原理可以概括为用掩码语言模型和后据预测两个无监督目标函数来训练转换器模型。
4.BERT模型可借鉴的核心思想
  ：转换器模型的思想可以被用在多种有序数据特征提取的建模中，特别是对于短文本的建模。
   从本质上说，其打破了CNN和RNN只能捕获单一的近邻关系和序关系的能力边界，利用注意力机制的优势来学不同位置上、不同尺度下的关系。
   值得注意的是，这儿注意力机制可以学到不同输入下不同尺度的不同的关系(动态关系)，而CNN学到的关系相对于位置而言确实固定不变的（filter的值学完后即固定不变），
   RNN学到的关系是考虑不到复杂的上下文关系的（Memory中的值高度依赖上一状态）。
  ：BERT在设计mask方法的时以及设计后句预测目标函数时，充分考虑到了所学到的表示可能用于的后续工作的特点。这种设计思想对于一种通用表示的无监督目标函数设计来说非常重要。
  ：BERT的总体结构上非常简洁，无针对某一类情况特别的设计，但是确显示出非常强大的学习能力和性能。
   在解决实际问题中，面对的往往是非常大的复杂数据，简单模型往往表现出更好的性能。同时，模型的简洁也保证了其计算效率，是完成大规模数据处理的基础。
 5.BERT模型可扩展的工作
  ：对于长文本的表示。BERT模型以基于注意力机制的转换器作为基础，天生的一个缺陷是无法作用于长文本。对于长文本，单用当前的注意力机制是不足以捕获各种复杂的交互关系的。可能需要设计一个深度的注意力机制，层级化地捕获各种关系。
  :对于噪音的容忍能力。BERT模型适用于短文本，但是社交网络中或者日常通讯中适用的短文本往往包含大量的噪音（不规则表示、错别字等），如何让BERT模型进一步拥有噪音容忍的能力值得研究。
  :对于一词多义的表示。虽然通过其上下文可以在一定程度上缓解一词多义的影响，但是毕竟一词多义对于BERT模型的原始输入中词编码影响极大，需要加入一定的机制来解决一词多意的表示问题。
  :BERT模型利用第一位[CLS]的表示输出作为整句话的表示，进而进行文本分类任务。但是在预训练的时候，仅仅用后句预测作为目标函数来训练。可以看出，如果分类任务不是建立在语句间相互关系的基础上时，
   例如对欺诈文本的分类任务，由于预训练的目标和实际任务相差太大，可能会有不太理想的效果。因此，如何更有效的在特定任务上进行fine-tuning是值得研究的。
  
  
  
  
  
  
  
  
  
 
 
 
 
 
 
 
 
 
 

  
  
  
  
  
  
  
