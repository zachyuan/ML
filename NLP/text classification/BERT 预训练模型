1.数据集
数据集为IMDB 电影影评，总共有三个数据文件，数据集为IMDB 电影影评，总共有三个数据文件，
2.BERT预训练模型
2.1
BERT模型是谷歌提出的基于双向Transformer构建的语言模型。BERT模型和ELMo有大不同，在之前的预训练模型（包括word2vec，ELMo等）都会生成词向量，这种类别的预训练模型属于domain transfer。
而近一两年提出的ULMFiT，GPT，BERT等都属于模型迁移。
2.2
BERT 模型是将预训练模型和下游任务模型结合在一起的，也就是说在做下游任务时仍然是用BERT模型，而且天然支持文本分类任务，在做文本分类任务时不需要对模型做修改。
谷歌提供了下面七种预训练好的模型文件。选择BERT-Base，Uncased。下载下来之后是一个zip文件，解压后有ckpt文件，一个模型参数的json文件，一个词汇表txt文件。
在应用BERT模型之前，我们需要去github上下载开源代码，我们可以直接clone下来，在这里有一个run_classifier.py文件，在做文本分类项目时，我们需要修改这个文件，主要是添加我们的数据预处理类。
2.3
在run_classifier.py文件中有一个基类DataProcessor类，在这个基类中定义了一个读取文件的静态方法_read_tsv，四个分别获取训练集，验证集，测试集和标签的方法。
我们要定义自己的数据处理的类,
这里我们没有直接用基类中的静态方法_read_tsv，因为我们的csv文件是用逗号分隔的，因此就自己定义了一个_read_csv的方法，其余的方法就是读取训练集，验证集，测试集和标签。
在这里标签就是一个列表，将我们的类别标签放入就行。训练集，验证集和测试集都是返回一个InputExample对象的列表。InputExample是run_classifier.py中定义的一个类。
2.4
我们自定义的数据处理类中，训练集和验证集是保存在不同文件中的，因此我们需要将我们之前预处理好的数据提前分割成训练集和验证集，并存放在同一个文件夹下面，文件的名称要和类中方法里的名称相同。
我们已经准备好了我们的数据集，并定义好了数据处理类，此时我们需要将我们的数据处理类加入到run_classifier.py文件中的main函数下面的processors字典中
之后就可以直接执行run_classifier.py文件
task_name就是我们定义的数据处理类的键，BERT模型较大，加载时需要较大的内存，如果出现内存溢出的问题，可以适当的降低batch_size的值。
3. 增加验证集输出的指标值
目前验证集上的输出指标值只有loss和accuracy，如上图所示，然而在分类时，我们可能还需要看auc，recall，precision的值。增加几行代码就可以搞定
